{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ad7813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:35.077985Z",
     "start_time": "2022-01-08T10:29:35.072171Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "项目功能：输入关键字，生成藏头诗。\n",
    "项目配置：\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 禁用词，包含如下字符的唐诗将被忽略\n",
    "DISALLOWED_WORDS = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']\n",
    "# 句子最大长度\n",
    "MAX_LEN = 64 ## 如果只用64，可能有bug。因为标点符号也算字数，所以64会把七律都滤掉的。\n",
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY = 8\n",
    "# 训练的batch size\n",
    "BATCH_SIZE = 16\n",
    "# 数据集路径\n",
    "DATASET_PATH = './poetry.txt'\n",
    "# 每个epoch训练完成后，随机生成SHOW_NUM首古诗作为展示\n",
    "SHOW_NUM = 5\n",
    "# 共训练多少个epoch\n",
    "TRAIN_EPOCHS = 1\n",
    "# 最佳权重保存路径\n",
    "BEST_MODEL_PATH = './best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b64cf3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:37.709181Z",
     "start_time": "2022-01-08T10:29:35.080567Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建数据集\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7cb464e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:37.719035Z",
     "start_time": "2022-01-08T10:29:37.711997Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    分词器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_dict):\n",
    "        # 词->编号的映射\n",
    "        self.token_dict = token_dict\n",
    "        # 编号->词的映射\n",
    "        self.token_dict_rev = {value: key for key, value in self.token_dict.items()}\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = len(self.token_dict)\n",
    "\n",
    "    def id_to_token(self, token_id):\n",
    "        \"\"\"\n",
    "        给定一个编号，查找词汇表中对应的词\n",
    "        :param token_id: 带查找词的编号\n",
    "        :return: 编号对应的词\n",
    "        \"\"\"\n",
    "        return self.token_dict_rev[token_id]\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        给定一个词，查找它在词汇表中的编号\n",
    "        未找到则返回低频词[UNK]的编号\n",
    "        :param token: 带查找编号的词\n",
    "        :return: 词的编号\n",
    "        \"\"\"\n",
    "        return self.token_dict.get(token, self.token_dict['[UNK]'])\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        给定一个字符串s，在头尾分别加上标记开始和结束的特殊字符，并将它转成对应的编号序列\n",
    "        :param tokens: 待编码字符串\n",
    "        :return: 编号序列\n",
    "        \"\"\"\n",
    "        # 加上开始标记\n",
    "        token_ids = [self.token_to_id('[CLS]'), ]\n",
    "        # 加入字符串编号序列\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        # 加上结束标记\n",
    "        token_ids.append(self.token_to_id('[SEP]'))\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        给定一个编号序列，将它解码成字符串\n",
    "        :param token_ids: 待解码的编号序列\n",
    "        :return: 解码出的字符串\n",
    "        \"\"\"\n",
    "        # 起止标记字符特殊处理\n",
    "        spec_tokens = {'[CLS]', '[SEP]'}\n",
    "        # 保存解码出的字符的list\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            token = self.id_to_token(token_id)\n",
    "            if token in spec_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        # 拼接字符串\n",
    "        return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023ae5fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:37.725525Z",
     "start_time": "2022-01-08T10:29:37.722346Z"
    }
   },
   "outputs": [],
   "source": [
    "# 禁用词\n",
    "disallowed_words = DISALLOWED_WORDS\n",
    "# 句子最大长度\n",
    "max_len = MAX_LEN\n",
    "# 最小词频\n",
    "min_word_frequency = MIN_WORD_FREQUENCY\n",
    "# mini batch 大小\n",
    "batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f79be0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:37.782194Z",
     "start_time": "2022-01-08T10:29:37.727865Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    # 将冒号统一成相同格式\n",
    "    lines = [line.replace('：', ':') for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8c7a29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:37.918527Z",
     "start_time": "2022-01-08T10:29:37.799940Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集列表\n",
    "poetry = []\n",
    "# 逐行处理读取到的数据\n",
    "for line in lines:\n",
    "    # 有且只能有一个冒号用来分割标题\n",
    "    if line.count(':') != 1:\n",
    "        continue\n",
    "    # 后半部分不能包含禁止词\n",
    "    __, last_part = line.split(':') ## 这个就是标题后面的内容，也就是诗的主体\n",
    "    ignore_flag = False\n",
    "    for dis_word in disallowed_words: ## 分析disallowed_word和ignore_flag，也就是说，但凡诗的主体内容包含disallowword，这行诗就不要了。\n",
    "        if dis_word in last_part:\n",
    "            ignore_flag = True\n",
    "            break\n",
    "    if ignore_flag:\n",
    "        continue\n",
    "    # 长度不能超过最大长度\n",
    "    if len(last_part) > max_len - 2: ## \n",
    "        continue\n",
    "    poetry.append(last_part.replace('\\n', '')) \n",
    "    ## 我呵呵，合着这么些过程就是为了筛选合适的诗歌啊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb43fb6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.115396Z",
     "start_time": "2022-01-08T10:29:37.931571Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3fd750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.155395Z",
     "start_time": "2022-01-08T10:29:38.151681Z"
    }
   },
   "outputs": [],
   "source": [
    "# 过滤掉低频词\n",
    "_tokens = [(token, count) for token, count in counter.items() if count >= min_word_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0394e2da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.161703Z",
     "start_time": "2022-01-08T10:29:38.158262Z"
    }
   },
   "outputs": [],
   "source": [
    "# 按词频排序\n",
    "_tokens = sorted(_tokens, key=lambda x: -x[1]) ## sorted是按照递增的顺序来的。如果要递减，就要加负号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4f812df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.166737Z",
     "start_time": "2022-01-08T10:29:38.163681Z"
    }
   },
   "outputs": [],
   "source": [
    "# 去掉词频，只保留词列表\n",
    "_tokens = [token for token, count in _tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d291f9b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.180136Z",
     "start_time": "2022-01-08T10:29:38.176645Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将特殊词和数据集中的词拼接起来\n",
    "_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + _tokens\n",
    "# 创建词典 token->id映射关系\n",
    "token_id_dict = dict(zip(_tokens, range(len(_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d465fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.185004Z",
     "start_time": "2022-01-08T10:29:38.182015Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用新词典重新建立分词器\n",
    "tokenizer = Tokenizer(token_id_dict)\n",
    "# 混洗数据\n",
    "np.random.shuffle(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0eb3ff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.197118Z",
     "start_time": "2022-01-08T10:29:38.187741Z"
    }
   },
   "outputs": [],
   "source": [
    "class PoetryDataGenerator:\n",
    "    \"\"\"\n",
    "    古诗训练数据集生成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, random=False):\n",
    "        # 数据集\n",
    "        self.data = data\n",
    "        # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # 每个epoch迭代的步数\n",
    "        self.steps = int(math.floor(len(self.data) / self.batch_size))\n",
    "        # 每个epoch开始时是否随机混洗\n",
    "        self.random = random\n",
    "\n",
    "    def sequence_padding(self, data, length=None, padding=None):\n",
    "        \"\"\"\n",
    "        将给定数据填充到相同长度\n",
    "        :param data: 待填充数据\n",
    "        :param length: 填充后的长度，不传递此参数则使用data中的最大长度\n",
    "        :param padding: 用于填充的数据，不传递此参数则使用[PAD]的对应编号\n",
    "        :return: 填充后的数据\n",
    "        \"\"\"\n",
    "        # 计算填充长度\n",
    "        if length is None:\n",
    "            length = max(map(len, data))\n",
    "        # 计算填充数据\n",
    "        if padding is None:\n",
    "            padding = tokenizer.token_to_id('[PAD]')\n",
    "        # 开始填充\n",
    "        outputs = []\n",
    "        for line in data:\n",
    "            padding_length = length - len(line)\n",
    "            # 不足就进行填充\n",
    "            if padding_length > 0:\n",
    "                outputs.append(np.concatenate([line, [padding] * padding_length]))\n",
    "            # 超过就进行截断\n",
    "            else:\n",
    "                outputs.append(line[:length])\n",
    "        return np.array(outputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        total = len(self.data)\n",
    "        # 是否随机混洗\n",
    "        if self.random:\n",
    "            np.random.shuffle(self.data)\n",
    "        # 迭代一个epoch，每次yield一个batch\n",
    "        for start in range(0, total, self.batch_size):\n",
    "            end = min(start + self.batch_size, total)\n",
    "            batch_data = []\n",
    "            # 逐一对古诗进行编码\n",
    "            for single_data in self.data[start:end]:\n",
    "                batch_data.append(tokenizer.encode(single_data))\n",
    "            # 填充为相同长度\n",
    "            batch_data = self.sequence_padding(batch_data, length=80)\n",
    "            # yield x,y\n",
    "            yield batch_data[:, :-1], tf.one_hot(batch_data[:, 1:], tokenizer.vocab_size)\n",
    "            del batch_data\n",
    "\n",
    "    def for_fit(self):\n",
    "        \"\"\"\n",
    "        创建一个生成器，用于训练\n",
    "        \"\"\"\n",
    "        # 死循环，当数据训练一个epoch之后，重新迭代数据\n",
    "        while True:\n",
    "            # 委托生成器\n",
    "            yield from self.__iter__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b23fbe84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.201580Z",
     "start_time": "2022-01-08T10:29:38.198949Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "data_generator = PoetryDataGenerator(poetry, random=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960719dc",
   "metadata": {},
   "source": [
    "## 真正的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "eb0cb509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T13:46:28.965226Z",
     "start_time": "2022-01-08T13:46:28.956273Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_acrostic(tokenizer, model, head, length_of_sentence = 5):\n",
    "    \"\"\"\n",
    "    随机生成一首藏头诗。\n",
    "    这一版是手动规定每一句长度的。\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 用于生成古诗的模型\n",
    "    :param head: 藏头诗的头\n",
    "    :return: 一个字符串，表示一首古诗\n",
    "    \"\"\"\n",
    "    # 使用空串初始化token_ids，加入[CLS]\n",
    "    token_ids = tokenizer.encode('')\n",
    "    token_ids = token_ids[:-1]\n",
    "    # 标点符号，这里简单的只把逗号和句号作为标点\n",
    "    punctuations = ['，', '。']\n",
    "    punctuation_ids = {tokenizer.token_to_id(token) for token in punctuations}\n",
    "    # 缓存生成的诗的list\n",
    "    total_poetry = []\n",
    "    poetry = []\n",
    "    # 对于藏头诗中的每一个字，都生成一个短句\n",
    "    for ch in head:\n",
    "        # 先记录下这个字\n",
    "        poetry.append(ch)\n",
    "        # 将藏头诗的字符转成token id\n",
    "        token_id = tokenizer.token_to_id(ch)\n",
    "        # 加入到列表中去\n",
    "        token_ids.append(token_id)\n",
    "        # 开始生成一个短句\n",
    "        \n",
    "        ### 加入一些规则噻。\n",
    "        ## 长度要固定。\n",
    "        ## 不能重字。\n",
    "        ### 平仄只能随缘了。这个强求不得。要么就增加训练轮数试试呗。\n",
    "        while not (   len(poetry) == length_of_sentence and len(set(token_ids)) == len(token_ids)    ):\n",
    "            # print(token_ids, set(token_ids))\n",
    "            # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\n",
    "            output = model(np.array([token_ids, ], dtype=np.int32))\n",
    "            ### output的形状是(1, 2, len(_tokens))\n",
    "            _probas = output.numpy()[0, -1, 3:]\n",
    "            del output\n",
    "            # 按照出现概率，对所有token倒序排列\n",
    "            p_args = _probas.argsort()[::-1][:100]\n",
    "            # 排列后的概率顺序\n",
    "            p = _probas[p_args]\n",
    "            # print(p_args, \"\\n\")\n",
    "            # 先对概率归一\n",
    "            p = p / sum(p) ## p的长度是100\n",
    "            # 再按照预测出的概率，随机选择一个词作为预测结果\n",
    "            \n",
    "            \n",
    "            while True:\n",
    "                target_index = np.random.choice(len(p), p=p)\n",
    "                target = p_args[target_index]\n",
    "                if target in ([0, 1, 2, 3] + list(punctuation_ids)): ## 不能是标点符号。\n",
    "                    continue\n",
    "                break\n",
    "                \n",
    "            token_ids.append(target)\n",
    "            poetry.append(tokenizer.id_to_token(target))\n",
    "        \n",
    "        total_poetry.extend(poetry)\n",
    "        poetry = []\n",
    "        token_ids = token_ids[:1]\n",
    "    \n",
    "    actual_poetry = []\n",
    "    for i, ch in enumerate(total_poetry):\n",
    "        actual_poetry.append(ch)\n",
    "        if ((i + 1) % length_of_sentence == 0) and ((i + 1) % (2 * length_of_sentence) != 0): ## 补逗号\n",
    "            actual_poetry.append(\"，\")\n",
    "        elif ((i + 1) % length_of_sentence == 0) and ((i + 1) % (2 * length_of_sentence) == 0): ## 补句号\n",
    "            actual_poetry.append(\"。\")\n",
    "    return ''.join(actual_poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4b818921",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T13:46:29.832644Z",
     "start_time": "2022-01-08T13:46:29.645768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'芳去日是看，心悠得思地。玉里连别似，蕊与夜别流。'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_acrostic(tokenizer, model, head=\"芳心玉蕊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb04e35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.981259Z",
     "start_time": "2022-01-08T10:29:38.212810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         439552    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 3434)        442986    \n",
      "=================================================================\n",
      "Total params: 1,145,706\n",
      "Trainable params: 1,145,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "构建LSTM模型\n",
    "\n",
    "\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    # 不定长度的输入\n",
    "    tf.keras.layers.Input((None,)),\n",
    "    # 词嵌入层\n",
    "    tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128),\n",
    "    # 第一个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 第二个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 对每一个时间点的输出都做softmax，预测下一个词的概率\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tokenizer.vocab_size, activation='softmax')),\n",
    "])\n",
    "\n",
    "# 查看模型结构\n",
    "model.summary()\n",
    "# 配置优化器和损失函数\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f9d2cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:29:38.987018Z",
     "start_time": "2022-01-08T10:29:38.983019Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "模型训练\n",
    "\n",
    "\"\"\"\n",
    "class Evaluate(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    训练过程评估，在每个epoch训练完成后，保留最优权重，并随机生成SHOW_NUM首古诗展示\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 给loss赋一个较大的初始值\n",
    "        self.lowest = 1e10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 在每个epoch训练完成后调用\n",
    "        # 如果当前loss更低，就保存当前模型参数\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save(BEST_MODEL_PATH)\n",
    "        # 随机生成几首古体诗测试，查看训练效果\n",
    "        print(\"cun'h\")\n",
    "        for i in range(SHOW_NUM):\n",
    "            print(generate_acrostic(tokenizer, model, head=\"春花秋月\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed707add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:33:16.918590Z",
     "start_time": "2022-01-08T10:29:39.018781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiuminke/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534/1534 [==============================] - 216s 139ms/step - loss: 5.4706\n",
      "cun'h\n",
      "春里青台，花重得清，秋石处子，月一后，\n",
      "春北白玉，花入头寒，秋白不日，月将声出情。\n",
      "春清头，花道去。秋朝子相山，月重道年月，\n",
      "春十生，花人长，秋无已，月落云，\n",
      "春朝风，花此，秋情，月前后。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbeef5a1af0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "model.fit_generator(data_generator.for_fit(), steps_per_epoch=data_generator.steps,workers=-1,use_multiprocessing=True,epochs=TRAIN_EPOCHS,\n",
    "                    callbacks=[Evaluate()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c41a98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T10:33:43.197752Z",
     "start_time": "2022-01-08T10:33:16.921163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入关键字:\n",
      "物是人非\n",
      "物玉无，是东，人已尘。非见作知在。 \n",
      "\n",
      "物如事得客，是有月出道，人月有天云。非如后不人。 \n",
      "\n",
      "物前不三水。是天南更声，人时将同里，非得是来三开。 \n",
      "\n",
      "物我，是后，人相落后，非无思道， \n",
      "\n",
      "物来王，是为里，人似多玉明，非明明后。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "输入关键字，生成藏头诗\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = tf.keras.models.load_model(BEST_MODEL_PATH)\n",
    "\n",
    "keywords = input('输入关键字:\\n')\n",
    "\n",
    "\n",
    "# 生成藏头诗\n",
    "for i in range(SHOW_NUM):\n",
    "    print(generate_acrostic(tokenizer, model, head=keywords),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946b36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4e33c3f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T13:27:15.099324Z",
     "start_time": "2022-01-08T13:27:07.163207Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-8a32d358faaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_model_multiEpoch.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSHOW_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_acrostic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"芳心玉蕊\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-202-96f637ee39b5>\u001b[0m in \u001b[0;36mgenerate_acrostic\u001b[0;34m(tokenizer, model, head, length_of_sentence)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoetry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlength_of_sentence\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m### output的形状是(1, 2, len(_tokens))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0m_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    367\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m--> 414\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    415\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                 **gpu_lstm_kwargs)\n\u001b[1;32m   1248\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m             last_output, outputs, new_h, new_c, runtime = standard_lstm(\n\u001b[0m\u001b[1;32m   1250\u001b[0m                 **normal_lstm_kwargs)\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m   last_output, outputs, new_states = backend.rnn(\n\u001b[0m\u001b[1;32m   1380\u001b[0m       \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4497\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4499\u001b[0;31m       final_outputs = tf.compat.v1.while_loop(\n\u001b[0m\u001b[1;32m   4500\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4501\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2775\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2776\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   4483\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4484\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4485\u001b[0;31m         output, new_states = step_function(current_input,\n\u001b[0m\u001b[1;32m   4486\u001b[0m                                            tuple(states) + tuple(constants))\n\u001b[1;32m   4487\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0mc_tm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# previous carry state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_tm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_dense_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1979\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1980\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3652\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3653\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3654\u001b[0;31m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3655\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5689\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5690\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5691\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   5692\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5693\u001b[0m         transpose_b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"best_model_multiEpoch.h5\")\n",
    "for i in range(SHOW_NUM):\n",
    "    print(generate_acrostic(tokenizer, model, head=\"芳心玉蕊\"),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3524f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e83e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
