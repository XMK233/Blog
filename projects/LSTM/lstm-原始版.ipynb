{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ad7813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T07:47:16.210941Z",
     "start_time": "2022-01-08T07:47:16.206821Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "项目功能：输入关键字，生成藏头诗。\n",
    "项目配置：\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 禁用词，包含如下字符的唐诗将被忽略\n",
    "DISALLOWED_WORDS = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']\n",
    "# 句子最大长度\n",
    "MAX_LEN = 64 + 8 ## 如果只用64，可能有bug。因为标点符号也算字数，所以64会把七律也过滤掉的。\n",
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY = 8\n",
    "# 训练的batch size\n",
    "BATCH_SIZE = 16\n",
    "# 数据集路径\n",
    "DATASET_PATH = './poetry.txt'\n",
    "# 每个epoch训练完成后，随机生成SHOW_NUM首古诗作为展示\n",
    "SHOW_NUM = 5\n",
    "# 共训练多少个epoch\n",
    "TRAIN_EPOCHS = 1\n",
    "# 最佳权重保存路径\n",
    "BEST_MODEL_PATH = './best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46fb67f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T07:47:16.437279Z",
     "start_time": "2022-01-08T07:47:16.434501Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建数据集\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc20ad7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T07:47:16.618843Z",
     "start_time": "2022-01-08T07:47:16.611208Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    分词器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_dict):\n",
    "        # 词->编号的映射\n",
    "        self.token_dict = token_dict\n",
    "        # 编号->词的映射\n",
    "        self.token_dict_rev = {value: key for key, value in self.token_dict.items()}\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = len(self.token_dict)\n",
    "\n",
    "    def id_to_token(self, token_id):\n",
    "        \"\"\"\n",
    "        给定一个编号，查找词汇表中对应的词\n",
    "        :param token_id: 带查找词的编号\n",
    "        :return: 编号对应的词\n",
    "        \"\"\"\n",
    "        return self.token_dict_rev[token_id]\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        给定一个词，查找它在词汇表中的编号\n",
    "        未找到则返回低频词[UNK]的编号\n",
    "        :param token: 带查找编号的词\n",
    "        :return: 词的编号\n",
    "        \"\"\"\n",
    "        return self.token_dict.get(token, self.token_dict['[UNK]'])\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        给定一个字符串s，在头尾分别加上标记开始和结束的特殊字符，并将它转成对应的编号序列\n",
    "        :param tokens: 待编码字符串\n",
    "        :return: 编号序列\n",
    "        \"\"\"\n",
    "        # 加上开始标记\n",
    "        token_ids = [self.token_to_id('[CLS]'), ]\n",
    "        # 加入字符串编号序列\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        # 加上结束标记\n",
    "        token_ids.append(self.token_to_id('[SEP]'))\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        给定一个编号序列，将它解码成字符串\n",
    "        :param token_ids: 待解码的编号序列\n",
    "        :return: 解码出的字符串\n",
    "        \"\"\"\n",
    "        # 起止标记字符特殊处理\n",
    "        spec_tokens = {'[CLS]', '[SEP]'}\n",
    "        # 保存解码出的字符的list\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            token = self.id_to_token(token_id)\n",
    "            if token in spec_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        # 拼接字符串\n",
    "        return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c9a8b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T07:47:16.781323Z",
     "start_time": "2022-01-08T07:47:16.778260Z"
    }
   },
   "outputs": [],
   "source": [
    "# 禁用词\n",
    "disallowed_words = DISALLOWED_WORDS\n",
    "# 句子最大长度\n",
    "max_len = MAX_LEN\n",
    "# 最小词频\n",
    "min_word_frequency = MIN_WORD_FREQUENCY\n",
    "# mini batch 大小\n",
    "batch_size = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "992b445d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T07:47:18.741924Z",
     "start_time": "2022-01-08T07:47:18.689385Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    # 将冒号统一成相同格式\n",
    "    lines = [line.replace('：', ':') for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7122b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T07:47:29.031401Z",
     "start_time": "2022-01-08T07:47:28.914704Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集列表\n",
    "poetry = []\n",
    "# 逐行处理读取到的数据\n",
    "for line in lines:\n",
    "    # 有且只能有一个冒号用来分割标题\n",
    "    if line.count(':') != 1:\n",
    "        continue\n",
    "    # 后半部分不能包含禁止词\n",
    "    __, last_part = line.split(':') ## 这个就是标题后面的内容，也就是诗的主体\n",
    "    ignore_flag = False\n",
    "    for dis_word in disallowed_words: ## 分析disallowed_word和ignore_flag，也就是说，但凡诗的主体内容包含disallowword，这行诗就不要了。\n",
    "        if dis_word in last_part:\n",
    "            ignore_flag = True\n",
    "            break\n",
    "    if ignore_flag:\n",
    "        continue\n",
    "    # 长度不能超过最大长度\n",
    "    if len(last_part) > max_len - 2: ## \n",
    "        continue\n",
    "    poetry.append(last_part.replace('\\n', '')) \n",
    "    ## 我呵呵，合着这么些过程就是为了筛选合适的诗歌啊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45d233ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T08:03:22.906657Z",
     "start_time": "2022-01-08T08:03:22.700331Z"
    }
   },
   "outputs": [],
   "source": [
    "# 统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8adccb42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T08:06:36.626743Z",
     "start_time": "2022-01-08T08:06:36.622882Z"
    }
   },
   "outputs": [],
   "source": [
    "# 过滤掉低频词\n",
    "_tokens = [(token, count) for token, count in counter.items() if count >= min_word_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65e03914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T08:07:25.405744Z",
     "start_time": "2022-01-08T08:07:25.401957Z"
    }
   },
   "outputs": [],
   "source": [
    "# 按词频排序\n",
    "_tokens = sorted(_tokens, key=lambda x: -x[1]) ## sorted是按照递增的顺序来的。如果要递减，就要加负号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfab53d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T08:09:42.340843Z",
     "start_time": "2022-01-08T08:09:42.337989Z"
    }
   },
   "outputs": [],
   "source": [
    "# 去掉词频，只保留词列表\n",
    "_tokens = [token for token, count in _tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e785db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T08:14:38.641074Z",
     "start_time": "2022-01-08T08:14:38.637602Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将特殊词和数据集中的词拼接起来\n",
    "_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + _tokens\n",
    "# 创建词典 token->id映射关系\n",
    "token_id_dict = dict(zip(_tokens, range(len(_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6634180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用新词典重新建立分词器\n",
    "tokenizer = Tokenizer(token_id_dict)\n",
    "# 混洗数据\n",
    "np.random.shuffle(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0eb3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryDataGenerator:\n",
    "    \"\"\"\n",
    "    古诗训练数据集生成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, random=False):\n",
    "        # 数据集\n",
    "        self.data = data\n",
    "        # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # 每个epoch迭代的步数\n",
    "        self.steps = int(math.floor(len(self.data) / self.batch_size))\n",
    "        # 每个epoch开始时是否随机混洗\n",
    "        self.random = random\n",
    "\n",
    "    def sequence_padding(self, data, length=None, padding=None):\n",
    "        \"\"\"\n",
    "        将给定数据填充到相同长度\n",
    "        :param data: 待填充数据\n",
    "        :param length: 填充后的长度，不传递此参数则使用data中的最大长度\n",
    "        :param padding: 用于填充的数据，不传递此参数则使用[PAD]的对应编号\n",
    "        :return: 填充后的数据\n",
    "        \"\"\"\n",
    "        # 计算填充长度\n",
    "        if length is None:\n",
    "            length = max(map(len, data))\n",
    "        # 计算填充数据\n",
    "        if padding is None:\n",
    "            padding = tokenizer.token_to_id('[PAD]')\n",
    "        # 开始填充\n",
    "        outputs = []\n",
    "        for line in data:\n",
    "            padding_length = length - len(line)\n",
    "            # 不足就进行填充\n",
    "            if padding_length > 0:\n",
    "                outputs.append(np.concatenate([line, [padding] * padding_length]))\n",
    "            # 超过就进行截断\n",
    "            else:\n",
    "                outputs.append(line[:length])\n",
    "        return np.array(outputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        total = len(self.data)\n",
    "        # 是否随机混洗\n",
    "        if self.random:\n",
    "            np.random.shuffle(self.data)\n",
    "        # 迭代一个epoch，每次yield一个batch\n",
    "        for start in range(0, total, self.batch_size):\n",
    "            end = min(start + self.batch_size, total)\n",
    "            batch_data = []\n",
    "            # 逐一对古诗进行编码\n",
    "            for single_data in self.data[start:end]:\n",
    "                batch_data.append(tokenizer.encode(single_data))\n",
    "            # 填充为相同长度\n",
    "            batch_data = self.sequence_padding(batch_data, length=80)\n",
    "            # yield x,y\n",
    "            yield batch_data[:, :-1], tf.one_hot(batch_data[:, 1:], tokenizer.vocab_size)\n",
    "            del batch_data\n",
    "\n",
    "    def for_fit(self):\n",
    "        \"\"\"\n",
    "        创建一个生成器，用于训练\n",
    "        \"\"\"\n",
    "        # 死循环，当数据训练一个epoch之后，重新迭代数据\n",
    "        while True:\n",
    "            # 委托生成器\n",
    "            yield from self.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0cb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_acrostic(tokenizer, model, head):\n",
    "    \"\"\"\n",
    "    随机生成一首藏头诗\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 用于生成古诗的模型\n",
    "    :param head: 藏头诗的头\n",
    "    :return: 一个字符串，表示一首古诗\n",
    "    \"\"\"\n",
    "    # 使用空串初始化token_ids，加入[CLS]\n",
    "    token_ids = tokenizer.encode('')\n",
    "    token_ids = token_ids[:-1]\n",
    "    # 标点符号，这里简单的只把逗号和句号作为标点\n",
    "    punctuations = ['，', '。']\n",
    "    punctuation_ids = {tokenizer.token_to_id(token) for token in punctuations}\n",
    "    # 缓存生成的诗的list\n",
    "    poetry = []\n",
    "    # 对于藏头诗中的每一个字，都生成一个短句\n",
    "    for ch in head:\n",
    "        # 先记录下这个字\n",
    "        poetry.append(ch)\n",
    "        # 将藏头诗的字符转成token id\n",
    "        token_id = tokenizer.token_to_id(ch)\n",
    "        # 加入到列表中去\n",
    "        token_ids.append(token_id)\n",
    "        # 开始生成一个短句\n",
    "        while True:\n",
    "            # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\n",
    "            output = model(np.array([token_ids, ], dtype=np.int32))\n",
    "            _probas = output.numpy()[0, -1, 3:]\n",
    "            del output\n",
    "            # 按照出现概率，对所有token倒序排列\n",
    "            p_args = _probas.argsort()[::-1][:100]\n",
    "            # 排列后的概率顺序\n",
    "            p = _probas[p_args]\n",
    "            # 先对概率归一\n",
    "            p = p / sum(p)\n",
    "            # 再按照预测出的概率，随机选择一个词作为预测结果\n",
    "            target_index = np.random.choice(len(p), p=p)\n",
    "            target = p_args[target_index] + 3\n",
    "            # 保存\n",
    "            token_ids.append(target)\n",
    "            # 只有不是特殊字符时，才保存到poetry里面去\n",
    "            if target > 3:\n",
    "                poetry.append(tokenizer.id_to_token(target))\n",
    "            if target in punctuation_ids:\n",
    "                break\n",
    "    return ''.join(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb04e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         439552    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 3434)        442986    \n",
      "=================================================================\n",
      "Total params: 1,145,706\n",
      "Trainable params: 1,145,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "构建LSTM模型\n",
    "\n",
    "\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    # 不定长度的输入\n",
    "    tf.keras.layers.Input((None,)),\n",
    "    # 词嵌入层\n",
    "    tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128),\n",
    "    # 第一个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 第二个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 对每一个时间点的输出都做softmax，预测下一个词的概率\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tokenizer.vocab_size, activation='softmax')),\n",
    "])\n",
    "\n",
    "# 查看模型结构\n",
    "model.summary()\n",
    "# 配置优化器和损失函数\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed707add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-a6c30794d98e>:30: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "1534/1534 [==============================] - ETA: 0s - loss: 4.9044cun'h\n",
      "春万中外客，花回下声水。秋秋云山日，月江月向回。\n",
      "春江千烟地，花风水青。秋三归月，月明前迟。\n",
      "春千山春，花南君然。秋山将无，月一不山。\n",
      "春子金相，花游草。秋中无雨，月鸟未人。\n",
      "春江烟色有去，花边见心云人中。秋日来事金飞寺，月高日家深开。\n",
      "1534/1534 [==============================] - 435s 284ms/step - loss: 4.9044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d7663f7a30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "模型训练\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Evaluate(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    训练过程评估，在每个epoch训练完成后，保留最优权重，并随机生成SHOW_NUM首古诗展示\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 给loss赋一个较大的初始值\n",
    "        self.lowest = 1e10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 在每个epoch训练完成后调用\n",
    "        # 如果当前loss更低，就保存当前模型参数\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save(BEST_MODEL_PATH)\n",
    "        # 随机生成几首古体诗测试，查看训练效果\n",
    "        print(\"cun'h\")\n",
    "        for i in range(SHOW_NUM):\n",
    "            print(generate_acrostic(tokenizer, model, head=\"春花秋月\"))\n",
    "\n",
    "# 创建数据集\n",
    "data_generator = PoetryDataGenerator(poetry, random=True)\n",
    "# 开始训练\n",
    "model.fit_generator(data_generator.for_fit(), steps_per_epoch=data_generator.steps,workers=-1,use_multiprocessing=True,epochs=TRAIN_EPOCHS,\n",
    "                    callbacks=[Evaluate()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c41a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入关键字:\n",
      "算法进阶\n",
      "算江有国，法朝长玉。进远无露，阶月夜寒。 \n",
      "\n",
      "算雨从无秋，法朝日人深。进楼雪上不，阶鸟孤门流。 \n",
      "\n",
      "算烟空此尽，法楼玉边人。进人清外水，阶此满似见。 \n",
      "\n",
      "算风天如去里，法别到见中日？相花飞月不水，进来长有同关。阶门一风去， \n",
      "\n",
      "算从旧事，法秋南生。进山人相客，阶为流城风。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "输入关键字，生成藏头诗\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = tf.keras.models.load_model(BEST_MODEL_PATH)\n",
    "\n",
    "keywords = input('输入关键字:\\n')\n",
    "\n",
    "\n",
    "# 生成藏头诗\n",
    "for i in range(SHOW_NUM):\n",
    "    print(generate_acrostic(tokenizer, model, head=keywords),'\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
